{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2,EfficientNetB0,EfficientNetB4\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26921 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n",
      "Found 1900 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = img_datagen.flow_from_directory('dataset/images/train/', class_mode = 'categorical', batch_size=32, target_size=(48,48), color_mode='grayscale')\n",
    "val_ds = img_datagen.flow_from_directory('dataset/images/validation/', class_mode = 'categorical', batch_size=32, target_size=(48,48), color_mode='grayscale')\n",
    "test_ds = img_datagen.flow_from_directory('dataset/images/test/', class_mode = 'categorical', batch_size=32, target_size=(48,48), color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 48, 48, 1)\n",
      "(32, 7)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_best = tf.keras.callbacks.ModelCheckpoint(\"Model.h5\",monitor='val_accuracy',save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 2.7368 - accuracy: 0.1942\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25177, saving model to Model.h5\n",
      "421/421 [==============================] - 128s 296ms/step - loss: 2.7368 - accuracy: 0.1942 - val_loss: 1.9295 - val_accuracy: 0.2518\n",
      "Epoch 2/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 2.3871 - accuracy: 0.2280\n",
      "Epoch 2: val_accuracy improved from 0.25177 to 0.32522, saving model to Model.h5\n",
      "421/421 [==============================] - 22s 51ms/step - loss: 2.3871 - accuracy: 0.2280 - val_loss: 1.7564 - val_accuracy: 0.3252\n",
      "Epoch 3/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 2.1828 - accuracy: 0.2579\n",
      "Epoch 3: val_accuracy improved from 0.32522 to 0.34376, saving model to Model.h5\n",
      "421/421 [==============================] - 22s 52ms/step - loss: 2.1828 - accuracy: 0.2579 - val_loss: 1.6833 - val_accuracy: 0.3438\n",
      "Epoch 4/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 2.0939 - accuracy: 0.2710\n",
      "Epoch 4: val_accuracy improved from 0.34376 to 0.35650, saving model to Model.h5\n",
      "421/421 [==============================] - 22s 52ms/step - loss: 2.0939 - accuracy: 0.2710 - val_loss: 1.6547 - val_accuracy: 0.3565\n",
      "Epoch 5/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.9863 - accuracy: 0.2904\n",
      "Epoch 5: val_accuracy improved from 0.35650 to 0.36923, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.9862 - accuracy: 0.2903 - val_loss: 1.6149 - val_accuracy: 0.3692\n",
      "Epoch 6/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.9031 - accuracy: 0.3099\n",
      "Epoch 6: val_accuracy improved from 0.36923 to 0.38919, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.9035 - accuracy: 0.3098 - val_loss: 1.5823 - val_accuracy: 0.3892\n",
      "Epoch 7/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.8405 - accuracy: 0.3233\n",
      "Epoch 7: val_accuracy improved from 0.38919 to 0.40065, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.8405 - accuracy: 0.3233 - val_loss: 1.5631 - val_accuracy: 0.4007\n",
      "Epoch 8/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.7785 - accuracy: 0.3400\n",
      "Epoch 8: val_accuracy improved from 0.40065 to 0.40504, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.7785 - accuracy: 0.3400 - val_loss: 1.5353 - val_accuracy: 0.4050\n",
      "Epoch 9/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.7254 - accuracy: 0.3554\n",
      "Epoch 9: val_accuracy improved from 0.40504 to 0.41367, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.7254 - accuracy: 0.3554 - val_loss: 1.5120 - val_accuracy: 0.4137\n",
      "Epoch 10/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.6716 - accuracy: 0.3697\n",
      "Epoch 10: val_accuracy improved from 0.41367 to 0.42881, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.6720 - accuracy: 0.3697 - val_loss: 1.4827 - val_accuracy: 0.4288\n",
      "Epoch 11/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.6268 - accuracy: 0.3840\n",
      "Epoch 11: val_accuracy did not improve from 0.42881\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.6267 - accuracy: 0.3840 - val_loss: 1.4850 - val_accuracy: 0.4261\n",
      "Epoch 12/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.5974 - accuracy: 0.3934\n",
      "Epoch 12: val_accuracy improved from 0.42881 to 0.45174, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.5974 - accuracy: 0.3934 - val_loss: 1.4353 - val_accuracy: 0.4517\n",
      "Epoch 13/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.5608 - accuracy: 0.4039\n",
      "Epoch 13: val_accuracy did not improve from 0.45174\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.5608 - accuracy: 0.4039 - val_loss: 1.4476 - val_accuracy: 0.4384\n",
      "Epoch 14/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.4139\n",
      "Epoch 14: val_accuracy improved from 0.45174 to 0.47170, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.5266 - accuracy: 0.4142 - val_loss: 1.3946 - val_accuracy: 0.4717\n",
      "Epoch 15/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.4979 - accuracy: 0.4261\n",
      "Epoch 15: val_accuracy did not improve from 0.47170\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.4980 - accuracy: 0.4259 - val_loss: 1.3968 - val_accuracy: 0.4662\n",
      "Epoch 16/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.4828 - accuracy: 0.4331\n",
      "Epoch 16: val_accuracy improved from 0.47170 to 0.48033, saving model to Model.h5\n",
      "421/421 [==============================] - 22s 53ms/step - loss: 1.4828 - accuracy: 0.4331 - val_loss: 1.3628 - val_accuracy: 0.4803\n",
      "Epoch 17/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.4583 - accuracy: 0.4384\n",
      "Epoch 17: val_accuracy improved from 0.48033 to 0.48712, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.4583 - accuracy: 0.4384 - val_loss: 1.3607 - val_accuracy: 0.4871\n",
      "Epoch 18/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.4279 - accuracy: 0.4512\n",
      "Epoch 18: val_accuracy improved from 0.48712 to 0.48825, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.4281 - accuracy: 0.4511 - val_loss: 1.3514 - val_accuracy: 0.4883\n",
      "Epoch 19/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.4183 - accuracy: 0.4585\n",
      "Epoch 19: val_accuracy did not improve from 0.48825\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.4183 - accuracy: 0.4585 - val_loss: 1.3558 - val_accuracy: 0.4851\n",
      "Epoch 20/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.4084 - accuracy: 0.4609\n",
      "Epoch 20: val_accuracy improved from 0.48825 to 0.49958, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.4087 - accuracy: 0.4607 - val_loss: 1.3208 - val_accuracy: 0.4996\n",
      "Epoch 21/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.3851 - accuracy: 0.4696\n",
      "Epoch 21: val_accuracy improved from 0.49958 to 0.50877, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.3851 - accuracy: 0.4696 - val_loss: 1.2935 - val_accuracy: 0.5088\n",
      "Epoch 22/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.3732 - accuracy: 0.4718\n",
      "Epoch 22: val_accuracy did not improve from 0.50877\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.3730 - accuracy: 0.4718 - val_loss: 1.2921 - val_accuracy: 0.5028\n",
      "Epoch 23/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.3710 - accuracy: 0.4765\n",
      "Epoch 23: val_accuracy did not improve from 0.50877\n",
      "421/421 [==============================] - 20s 49ms/step - loss: 1.3710 - accuracy: 0.4765 - val_loss: 1.2971 - val_accuracy: 0.5076\n",
      "Epoch 24/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.3467 - accuracy: 0.4869\n",
      "Epoch 24: val_accuracy improved from 0.50877 to 0.52024, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.3464 - accuracy: 0.4870 - val_loss: 1.2686 - val_accuracy: 0.5202\n",
      "Epoch 25/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.3364 - accuracy: 0.4872\n",
      "Epoch 25: val_accuracy did not improve from 0.52024\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.3364 - accuracy: 0.4872 - val_loss: 1.2642 - val_accuracy: 0.5181\n",
      "Epoch 26/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.3308 - accuracy: 0.4911\n",
      "Epoch 26: val_accuracy did not improve from 0.52024\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.3306 - accuracy: 0.4913 - val_loss: 1.2704 - val_accuracy: 0.5184\n",
      "Epoch 27/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.3205 - accuracy: 0.4954\n",
      "Epoch 27: val_accuracy improved from 0.52024 to 0.52646, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.3207 - accuracy: 0.4952 - val_loss: 1.2441 - val_accuracy: 0.5265\n",
      "Epoch 28/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.3126 - accuracy: 0.4990\n",
      "Epoch 28: val_accuracy improved from 0.52646 to 0.52816, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.3126 - accuracy: 0.4990 - val_loss: 1.2329 - val_accuracy: 0.5282\n",
      "Epoch 29/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.3055 - accuracy: 0.5019\n",
      "Epoch 29: val_accuracy improved from 0.52816 to 0.53057, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.3055 - accuracy: 0.5019 - val_loss: 1.2328 - val_accuracy: 0.5306\n",
      "Epoch 30/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.2956 - accuracy: 0.5046\n",
      "Epoch 30: val_accuracy improved from 0.53057 to 0.53920, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.2956 - accuracy: 0.5046 - val_loss: 1.2131 - val_accuracy: 0.5392\n",
      "Epoch 31/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.2889 - accuracy: 0.5107\n",
      "Epoch 31: val_accuracy did not improve from 0.53920\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.2889 - accuracy: 0.5107 - val_loss: 1.2128 - val_accuracy: 0.5371\n",
      "Epoch 32/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.2800 - accuracy: 0.5139\n",
      "Epoch 32: val_accuracy improved from 0.53920 to 0.54118, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.2802 - accuracy: 0.5138 - val_loss: 1.2019 - val_accuracy: 0.5412\n",
      "Epoch 33/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.2762 - accuracy: 0.5137\n",
      "Epoch 33: val_accuracy did not improve from 0.54118\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.2762 - accuracy: 0.5135 - val_loss: 1.2354 - val_accuracy: 0.5217\n",
      "Epoch 34/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.2737 - accuracy: 0.5139\n",
      "Epoch 34: val_accuracy improved from 0.54118 to 0.54543, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.2737 - accuracy: 0.5139 - val_loss: 1.1892 - val_accuracy: 0.5454\n",
      "Epoch 35/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.2581 - accuracy: 0.5211\n",
      "Epoch 35: val_accuracy did not improve from 0.54543\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.2589 - accuracy: 0.5206 - val_loss: 1.2100 - val_accuracy: 0.5396\n",
      "Epoch 36/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.2594 - accuracy: 0.5179\n",
      "Epoch 36: val_accuracy improved from 0.54543 to 0.55208, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.2598 - accuracy: 0.5176 - val_loss: 1.1824 - val_accuracy: 0.5521\n",
      "Epoch 37/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.2581 - accuracy: 0.5215\n",
      "Epoch 37: val_accuracy did not improve from 0.55208\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.2581 - accuracy: 0.5214 - val_loss: 1.2029 - val_accuracy: 0.5379\n",
      "Epoch 38/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.2417 - accuracy: 0.5278\n",
      "Epoch 38: val_accuracy did not improve from 0.55208\n",
      "421/421 [==============================] - 20s 49ms/step - loss: 1.2417 - accuracy: 0.5278 - val_loss: 1.1861 - val_accuracy: 0.5460\n",
      "Epoch 39/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.2421 - accuracy: 0.5299\n",
      "Epoch 39: val_accuracy improved from 0.55208 to 0.55802, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 51ms/step - loss: 1.2421 - accuracy: 0.5299 - val_loss: 1.1617 - val_accuracy: 0.5580\n",
      "Epoch 40/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.2317 - accuracy: 0.5337\n",
      "Epoch 40: val_accuracy improved from 0.55802 to 0.56425, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.2320 - accuracy: 0.5335 - val_loss: 1.1471 - val_accuracy: 0.5643\n",
      "Epoch 41/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.2333 - accuracy: 0.5325\n",
      "Epoch 41: val_accuracy improved from 0.56425 to 0.56779, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.2333 - accuracy: 0.5325 - val_loss: 1.1468 - val_accuracy: 0.5678\n",
      "Epoch 42/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.2281 - accuracy: 0.5358\n",
      "Epoch 42: val_accuracy did not improve from 0.56779\n",
      "421/421 [==============================] - 22s 52ms/step - loss: 1.2281 - accuracy: 0.5359 - val_loss: 1.1423 - val_accuracy: 0.5650\n",
      "Epoch 43/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.2233 - accuracy: 0.5345\n",
      "Epoch 43: val_accuracy did not improve from 0.56779\n",
      "421/421 [==============================] - 21s 51ms/step - loss: 1.2235 - accuracy: 0.5346 - val_loss: 1.1725 - val_accuracy: 0.5558\n",
      "Epoch 44/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.2233 - accuracy: 0.5365\n",
      "Epoch 44: val_accuracy did not improve from 0.56779\n",
      "421/421 [==============================] - 22s 52ms/step - loss: 1.2234 - accuracy: 0.5365 - val_loss: 1.1431 - val_accuracy: 0.5609\n",
      "Epoch 45/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.2173 - accuracy: 0.5373\n",
      "Epoch 45: val_accuracy did not improve from 0.56779\n",
      "421/421 [==============================] - 22s 52ms/step - loss: 1.2173 - accuracy: 0.5373 - val_loss: 1.1500 - val_accuracy: 0.5621\n",
      "Epoch 46/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.2069 - accuracy: 0.5441\n",
      "Epoch 46: val_accuracy improved from 0.56779 to 0.56793, saving model to Model.h5\n",
      "421/421 [==============================] - 22s 51ms/step - loss: 1.2068 - accuracy: 0.5442 - val_loss: 1.1371 - val_accuracy: 0.5679\n",
      "Epoch 47/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.2082 - accuracy: 0.5411\n",
      "Epoch 47: val_accuracy did not improve from 0.56793\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.2082 - accuracy: 0.5411 - val_loss: 1.1393 - val_accuracy: 0.5652\n",
      "Epoch 48/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.2016 - accuracy: 0.5442\n",
      "Epoch 48: val_accuracy did not improve from 0.56793\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.2012 - accuracy: 0.5443 - val_loss: 1.1298 - val_accuracy: 0.5641\n",
      "Epoch 49/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1978 - accuracy: 0.5438\n",
      "Epoch 49: val_accuracy improved from 0.56793 to 0.57671, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.1978 - accuracy: 0.5438 - val_loss: 1.1203 - val_accuracy: 0.5767\n",
      "Epoch 50/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1972 - accuracy: 0.5449\n",
      "Epoch 50: val_accuracy did not improve from 0.57671\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1972 - accuracy: 0.5449 - val_loss: 1.1531 - val_accuracy: 0.5617\n",
      "Epoch 51/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1926 - accuracy: 0.5483\n",
      "Epoch 51: val_accuracy did not improve from 0.57671\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1923 - accuracy: 0.5484 - val_loss: 1.1309 - val_accuracy: 0.5701\n",
      "Epoch 52/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1868 - accuracy: 0.5491\n",
      "Epoch 52: val_accuracy did not improve from 0.57671\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1864 - accuracy: 0.5493 - val_loss: 1.1531 - val_accuracy: 0.5531\n",
      "Epoch 53/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1886 - accuracy: 0.5520\n",
      "Epoch 53: val_accuracy improved from 0.57671 to 0.57770, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1886 - accuracy: 0.5520 - val_loss: 1.1185 - val_accuracy: 0.5777\n",
      "Epoch 54/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1807 - accuracy: 0.5520\n",
      "Epoch 54: val_accuracy did not improve from 0.57770\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1807 - accuracy: 0.5520 - val_loss: 1.1481 - val_accuracy: 0.5647\n",
      "Epoch 55/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1836 - accuracy: 0.5526\n",
      "Epoch 55: val_accuracy did not improve from 0.57770\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1836 - accuracy: 0.5526 - val_loss: 1.1118 - val_accuracy: 0.5730\n",
      "Epoch 56/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1762 - accuracy: 0.5531\n",
      "Epoch 56: val_accuracy improved from 0.57770 to 0.58109, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1762 - accuracy: 0.5531 - val_loss: 1.1004 - val_accuracy: 0.5811\n",
      "Epoch 57/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1708 - accuracy: 0.5539\n",
      "Epoch 57: val_accuracy improved from 0.58109 to 0.58208, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1709 - accuracy: 0.5538 - val_loss: 1.1049 - val_accuracy: 0.5821\n",
      "Epoch 58/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1670 - accuracy: 0.5557\n",
      "Epoch 58: val_accuracy improved from 0.58208 to 0.58435, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1668 - accuracy: 0.5557 - val_loss: 1.0975 - val_accuracy: 0.5843\n",
      "Epoch 59/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1637 - accuracy: 0.5603\n",
      "Epoch 59: val_accuracy did not improve from 0.58435\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.1638 - accuracy: 0.5603 - val_loss: 1.1371 - val_accuracy: 0.5661\n",
      "Epoch 60/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1694 - accuracy: 0.5559\n",
      "Epoch 60: val_accuracy did not improve from 0.58435\n",
      "421/421 [==============================] - 22s 51ms/step - loss: 1.1696 - accuracy: 0.5558 - val_loss: 1.1019 - val_accuracy: 0.5774\n",
      "Epoch 61/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1597 - accuracy: 0.5561\n",
      "Epoch 61: val_accuracy did not improve from 0.58435\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.1597 - accuracy: 0.5561 - val_loss: 1.1024 - val_accuracy: 0.5790\n",
      "Epoch 62/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1588 - accuracy: 0.5627\n",
      "Epoch 62: val_accuracy did not improve from 0.58435\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1588 - accuracy: 0.5627 - val_loss: 1.0907 - val_accuracy: 0.5843\n",
      "Epoch 63/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1528 - accuracy: 0.5643\n",
      "Epoch 63: val_accuracy did not improve from 0.58435\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.1528 - accuracy: 0.5643 - val_loss: 1.1474 - val_accuracy: 0.5592\n",
      "Epoch 64/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1600 - accuracy: 0.5628\n",
      "Epoch 64: val_accuracy improved from 0.58435 to 0.59072, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.1600 - accuracy: 0.5628 - val_loss: 1.0805 - val_accuracy: 0.5907\n",
      "Epoch 65/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1512 - accuracy: 0.5631\n",
      "Epoch 65: val_accuracy did not improve from 0.59072\n",
      "421/421 [==============================] - 21s 51ms/step - loss: 1.1512 - accuracy: 0.5631 - val_loss: 1.0925 - val_accuracy: 0.5866\n",
      "Epoch 66/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1534 - accuracy: 0.5615\n",
      "Epoch 66: val_accuracy did not improve from 0.59072\n",
      "421/421 [==============================] - 22s 51ms/step - loss: 1.1535 - accuracy: 0.5616 - val_loss: 1.0887 - val_accuracy: 0.5860\n",
      "Epoch 67/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1418 - accuracy: 0.5677\n",
      "Epoch 67: val_accuracy did not improve from 0.59072\n",
      "421/421 [==============================] - 23s 54ms/step - loss: 1.1415 - accuracy: 0.5680 - val_loss: 1.0835 - val_accuracy: 0.5856\n",
      "Epoch 68/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1415 - accuracy: 0.5659\n",
      "Epoch 68: val_accuracy did not improve from 0.59072\n",
      "421/421 [==============================] - 21s 51ms/step - loss: 1.1415 - accuracy: 0.5658 - val_loss: 1.0836 - val_accuracy: 0.5849\n",
      "Epoch 69/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1444 - accuracy: 0.5663\n",
      "Epoch 69: val_accuracy did not improve from 0.59072\n",
      "421/421 [==============================] - 21s 51ms/step - loss: 1.1444 - accuracy: 0.5663 - val_loss: 1.0780 - val_accuracy: 0.5897\n",
      "Epoch 70/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1360 - accuracy: 0.5695\n",
      "Epoch 70: val_accuracy did not improve from 0.59072\n",
      "421/421 [==============================] - 22s 51ms/step - loss: 1.1360 - accuracy: 0.5695 - val_loss: 1.0836 - val_accuracy: 0.5870\n",
      "Epoch 71/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1346 - accuracy: 0.5722\n",
      "Epoch 71: val_accuracy did not improve from 0.59072\n",
      "421/421 [==============================] - 22s 53ms/step - loss: 1.1346 - accuracy: 0.5722 - val_loss: 1.0947 - val_accuracy: 0.5838\n",
      "Epoch 72/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1351 - accuracy: 0.5672\n",
      "Epoch 72: val_accuracy improved from 0.59072 to 0.59241, saving model to Model.h5\n",
      "421/421 [==============================] - 22s 51ms/step - loss: 1.1351 - accuracy: 0.5672 - val_loss: 1.0710 - val_accuracy: 0.5924\n",
      "Epoch 73/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1317 - accuracy: 0.5674\n",
      "Epoch 73: val_accuracy did not improve from 0.59241\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1317 - accuracy: 0.5674 - val_loss: 1.0734 - val_accuracy: 0.5906\n",
      "Epoch 74/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1364 - accuracy: 0.5714\n",
      "Epoch 74: val_accuracy improved from 0.59241 to 0.59524, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1364 - accuracy: 0.5714 - val_loss: 1.0657 - val_accuracy: 0.5952\n",
      "Epoch 75/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1293 - accuracy: 0.5717\n",
      "Epoch 75: val_accuracy did not improve from 0.59524\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1291 - accuracy: 0.5717 - val_loss: 1.0811 - val_accuracy: 0.5841\n",
      "Epoch 76/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1299 - accuracy: 0.5708\n",
      "Epoch 76: val_accuracy did not improve from 0.59524\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1296 - accuracy: 0.5710 - val_loss: 1.0854 - val_accuracy: 0.5877\n",
      "Epoch 77/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1261 - accuracy: 0.5709\n",
      "Epoch 77: val_accuracy improved from 0.59524 to 0.59553, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.1260 - accuracy: 0.5709 - val_loss: 1.0727 - val_accuracy: 0.5955\n",
      "Epoch 78/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1218 - accuracy: 0.5778\n",
      "Epoch 78: val_accuracy did not improve from 0.59553\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1218 - accuracy: 0.5778 - val_loss: 1.1437 - val_accuracy: 0.5650\n",
      "Epoch 79/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1175 - accuracy: 0.5769\n",
      "Epoch 79: val_accuracy improved from 0.59553 to 0.59907, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1175 - accuracy: 0.5769 - val_loss: 1.0690 - val_accuracy: 0.5991\n",
      "Epoch 80/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1189 - accuracy: 0.5773\n",
      "Epoch 80: val_accuracy did not improve from 0.59907\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1189 - accuracy: 0.5773 - val_loss: 1.0820 - val_accuracy: 0.5928\n",
      "Epoch 81/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1171 - accuracy: 0.5816\n",
      "Epoch 81: val_accuracy did not improve from 0.59907\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.1171 - accuracy: 0.5816 - val_loss: 1.0719 - val_accuracy: 0.5955\n",
      "Epoch 82/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1103 - accuracy: 0.5783\n",
      "Epoch 82: val_accuracy improved from 0.59907 to 0.60473, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 51ms/step - loss: 1.1103 - accuracy: 0.5783 - val_loss: 1.0506 - val_accuracy: 0.6047\n",
      "Epoch 83/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1134 - accuracy: 0.5816\n",
      "Epoch 83: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.1134 - accuracy: 0.5816 - val_loss: 1.0845 - val_accuracy: 0.5886\n",
      "Epoch 84/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1084 - accuracy: 0.5812\n",
      "Epoch 84: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 51ms/step - loss: 1.1087 - accuracy: 0.5811 - val_loss: 1.0528 - val_accuracy: 0.6013\n",
      "Epoch 85/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1051 - accuracy: 0.5826\n",
      "Epoch 85: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.1053 - accuracy: 0.5824 - val_loss: 1.0706 - val_accuracy: 0.5931\n",
      "Epoch 86/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1033 - accuracy: 0.5828\n",
      "Epoch 86: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1033 - accuracy: 0.5827 - val_loss: 1.0601 - val_accuracy: 0.5989\n",
      "Epoch 87/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1017 - accuracy: 0.5853\n",
      "Epoch 87: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.1017 - accuracy: 0.5853 - val_loss: 1.0534 - val_accuracy: 0.6032\n",
      "Epoch 88/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1058 - accuracy: 0.5838\n",
      "Epoch 88: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1058 - accuracy: 0.5838 - val_loss: 1.0587 - val_accuracy: 0.6033\n",
      "Epoch 89/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.1015 - accuracy: 0.5838\n",
      "Epoch 89: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1016 - accuracy: 0.5836 - val_loss: 1.0460 - val_accuracy: 0.6044\n",
      "Epoch 90/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.0953 - accuracy: 0.5846\n",
      "Epoch 90: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.0950 - accuracy: 0.5846 - val_loss: 1.0732 - val_accuracy: 0.5892\n",
      "Epoch 91/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.0958 - accuracy: 0.5868\n",
      "Epoch 91: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.0958 - accuracy: 0.5868 - val_loss: 1.0486 - val_accuracy: 0.6037\n",
      "Epoch 92/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.1028 - accuracy: 0.5792\n",
      "Epoch 92: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.1028 - accuracy: 0.5792 - val_loss: 1.0809 - val_accuracy: 0.5981\n",
      "Epoch 93/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.0998 - accuracy: 0.5872\n",
      "Epoch 93: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.0998 - accuracy: 0.5872 - val_loss: 1.0562 - val_accuracy: 0.5993\n",
      "Epoch 94/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.0917 - accuracy: 0.5862\n",
      "Epoch 94: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.0917 - accuracy: 0.5862 - val_loss: 1.0598 - val_accuracy: 0.6020\n",
      "Epoch 95/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.0957 - accuracy: 0.5860\n",
      "Epoch 95: val_accuracy did not improve from 0.60473\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.0957 - accuracy: 0.5860 - val_loss: 1.0708 - val_accuracy: 0.5923\n",
      "Epoch 96/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.0948 - accuracy: 0.5877\n",
      "Epoch 96: val_accuracy improved from 0.60473 to 0.60529, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.0948 - accuracy: 0.5877 - val_loss: 1.0404 - val_accuracy: 0.6053\n",
      "Epoch 97/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.0880 - accuracy: 0.5895\n",
      "Epoch 97: val_accuracy did not improve from 0.60529\n",
      "421/421 [==============================] - 22s 51ms/step - loss: 1.0880 - accuracy: 0.5895 - val_loss: 1.0604 - val_accuracy: 0.6013\n",
      "Epoch 98/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.0926 - accuracy: 0.5854\n",
      "Epoch 98: val_accuracy did not improve from 0.60529\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.0923 - accuracy: 0.5855 - val_loss: 1.0459 - val_accuracy: 0.6036\n",
      "Epoch 99/100\n",
      "420/421 [============================>.] - ETA: 0s - loss: 1.0881 - accuracy: 0.5837\n",
      "Epoch 99: val_accuracy did not improve from 0.60529\n",
      "421/421 [==============================] - 21s 50ms/step - loss: 1.0883 - accuracy: 0.5838 - val_loss: 1.0626 - val_accuracy: 0.5996\n",
      "Epoch 100/100\n",
      "421/421 [==============================] - ETA: 0s - loss: 1.0849 - accuracy: 0.5905\n",
      "Epoch 100: val_accuracy improved from 0.60529 to 0.61025, saving model to Model.h5\n",
      "421/421 [==============================] - 21s 49ms/step - loss: 1.0849 - accuracy: 0.5905 - val_loss: 1.0353 - val_accuracy: 0.6102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a82b3f3ca0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=[save_best], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 12s 393ms/step - loss: 1.1957 - accuracy: 0.5558\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('filters_1', min_value=32, max_value=64, step=32),\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=(48, 48, 1)\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('filters_2', min_value=64, max_value=128, step=64),\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('filters_3', min_value=128, max_value=256, step=128),\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_3', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(\n",
    "        units=hp.Int('dense_units', min_value=128, max_value=512, step=128),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(hp.Float('dense_dropout', min_value=0.3, max_value=0.6, step=0.1)))\n",
    "    \n",
    "    model.add(layers.Dense(7, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=hp.Choice('optimizer', ['adam', 'sgd']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\hyperparameter_tuning\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    directory='my_dir',\n",
    "    project_name='hyperparameter_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Complete [00h 09m 18s]\n",
      "val_accuracy: 0.5355222225189209\n",
      "\n",
      "Best val_accuracy So Far: 0.5355222225189209\n",
      "Total elapsed time: 00h 57m 10s\n",
      "\n",
      "Search: Running Trial #8\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "32                |64                |filters_1\n",
      "0.4               |0.4               |dropout_1\n",
      "64                |64                |filters_2\n",
      "0.4               |0.2               |dropout_2\n",
      "128               |256               |filters_3\n",
      "0.2               |0.2               |dropout_3\n",
      "128               |256               |dense_units\n",
      "0.5               |0.4               |dense_dropout\n",
      "sgd               |adam              |optimizer\n",
      "\n",
      "Epoch 1/10\n",
      "842/842 [==============================] - 20s 23ms/step - loss: 2.1451 - accuracy: 0.2173 - val_loss: 1.9467 - val_accuracy: 0.2523\n",
      "Epoch 2/10\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 1.8029 - accuracy: 0.2660 - val_loss: 1.9474 - val_accuracy: 0.2593\n",
      "Epoch 3/10\n",
      "842/842 [==============================] - 19s 22ms/step - loss: 1.7508 - accuracy: 0.2896 - val_loss: 2.3316 - val_accuracy: 0.2787\n",
      "Epoch 4/10\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 1.7332 - accuracy: 0.2950 - val_loss: 1.9752 - val_accuracy: 0.3002\n",
      "Epoch 5/10\n",
      "842/842 [==============================] - 19s 23ms/step - loss: 1.7184 - accuracy: 0.3082 - val_loss: 1.8426 - val_accuracy: 0.3201\n",
      "Epoch 6/10\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 1.7027 - accuracy: 0.3179 - val_loss: 2.1956 - val_accuracy: 0.3405\n",
      "Epoch 7/10\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 1.6819 - accuracy: 0.3237 - val_loss: 1.8227 - val_accuracy: 0.3428\n",
      "Epoch 8/10\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 1.6647 - accuracy: 0.3362 - val_loss: 1.6634 - val_accuracy: 0.3491\n",
      "Epoch 9/10\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 1.6481 - accuracy: 0.3458 - val_loss: 1.7398 - val_accuracy: 0.3193\n",
      "Epoch 10/10\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 1.6243 - accuracy: 0.3618 - val_loss: 1.5593 - val_accuracy: 0.3920\n",
      "Epoch 1/10\n",
      "842/842 [==============================] - 19s 21ms/step - loss: 2.1325 - accuracy: 0.2150 - val_loss: 2.4119 - val_accuracy: 0.2290\n",
      "Epoch 2/10\n",
      "842/842 [==============================] - 18s 21ms/step - loss: 1.7934 - accuracy: 0.2699 - val_loss: 2.0380 - val_accuracy: 0.2604\n",
      "Epoch 3/10\n",
      "842/842 [==============================] - 18s 21ms/step - loss: 1.7507 - accuracy: 0.2875 - val_loss: 1.8151 - val_accuracy: 0.2990\n",
      "Epoch 4/10\n",
      "842/842 [==============================] - 18s 21ms/step - loss: 1.7279 - accuracy: 0.2983 - val_loss: 1.7052 - val_accuracy: 0.3036\n",
      "Epoch 5/10\n",
      "842/842 [==============================] - 18s 21ms/step - loss: 1.7134 - accuracy: 0.3079 - val_loss: 1.6763 - val_accuracy: 0.3331\n",
      "Epoch 6/10\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 1.6871 - accuracy: 0.3244 - val_loss: 1.6367 - val_accuracy: 0.3612\n",
      "Epoch 7/10\n",
      "841/842 [============================>.] - ETA: 0s - loss: 1.6696 - accuracy: 0.3330"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m best_model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_models(num_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m best_hyperparameters \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    242\u001b[0m     ):\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    255\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[0;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[1;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m }\n\u001b[0;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1947\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1949\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner.search(train_ds, validation_data=val_ds, epochs=10)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
